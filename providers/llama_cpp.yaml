provider_id: "llamacpp_local"
provider_type: "openai_compat"
resource_group: "local_gpu"
api:
  base_url: "http://127.0.0.1:8001"
  health:
    method: "GET"
    path: "/v1/models"
    success_codes: [200]
    timeout_seconds: 2
  models:
    method: "GET"
    path: "/v1/models"
detect:
  method: "none"
start:
  enabled: true
  command: "python"
  args: ["-m","llama_cpp.server","--host","127.0.0.1","--port","8001","--model","REPLACE_WITH_MODEL_PATH"]
  startup_grace_seconds: 30
stop:
  method: "terminate_process"
policy:
  keep_warm: false
  idle_shutdown_seconds: 60
  max_start_attempts: 2
  restart_on_failure: false
